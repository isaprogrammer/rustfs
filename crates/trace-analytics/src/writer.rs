use crate::error::Result;
use crate::schema::TraceRecord;
use bytes::Bytes;
use chrono::{Datelike, Timelike, Utc};
use crossbeam_queue::ArrayQueue;
use datafusion::arrow::array::*;
use datafusion::arrow::datatypes::*;
use datafusion::arrow::record_batch::RecordBatch;
use datafusion::parquet::arrow::ArrowWriter;
use datafusion::parquet::basic::{Compression, Encoding};
use datafusion::parquet::file::properties::WriterProperties;
use rustfs_ecstore::store_api::{ObjectIO, ObjectOptions, PutObjReader};
use rustfs_rio::{HashReader, WarpReader};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{Mutex, mpsc, oneshot};
use tracing::{error, info, warn};

/// Worker çº¿ç¨‹çš„æ§åˆ¶æ¶ˆæ¯æšä¸¾
/// ç”¨äºåœ¨ä¸»çº¿ç¨‹å’Œåå° worker ä¹‹é—´ä¼ é€’æŒ‡ä»¤
enum WorkerMsg {
    /// å†™å…¥ä¸€æ¡ trace è®°å½•
    Record(TraceRecord),
    /// å…³é—­ worker çš„ä¿¡å·
    Shutdown,
}

/// åˆ†å¸ƒå¼è¿½è¸ªæ•°æ®å†™å…¥å™¨
///
/// è®¾è®¡æ¨¡å¼ï¼š
/// - ä½¿ç”¨ MPSC channel å®ç°å¼‚æ­¥å†™å…¥é˜Ÿåˆ—
/// - åå° worker çº¿ç¨‹è´Ÿè´£æ‰¹é‡èšåˆå’Œå®šæœŸåˆ·ç›˜
/// - æ”¯æŒä¼˜é›…å…³é—­ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
///
/// æ€§èƒ½ç‰¹ç‚¹ï¼š
/// - æ‰¹é‡å†™å…¥å‡å°‘ I/O æ¬¡æ•°
/// - Parquet åˆ—å¼å­˜å‚¨æ ¼å¼ï¼Œé«˜å‹ç¼©æ¯”ï¼ˆSnappy å‹ç¼©ï¼‰
/// - å¯¹è±¡æ± å¤ç”¨ Builderï¼Œé›¶åˆ†é…å¼€é”€
/// - åŒç¼“å†²æœºåˆ¶ï¼Œflush ä¸é˜»å¡æ•°æ®æ¥æ”¶
/// - å­—å…¸ç¼–ç ä¼˜åŒ–é‡å¤å­—æ®µ
pub struct TraceWriter {
    /// å‘ worker å‘é€æ¶ˆæ¯çš„é€šé“ï¼ˆç”¨ Mutex åŒ…è£¹ä»¥æ”¯æŒä¼˜é›…å…³é—­ï¼‰
    tx: Mutex<Option<mpsc::Sender<WorkerMsg>>>,
    /// æ¥æ”¶ worker å®Œæˆä¿¡å·çš„é€šé“
    done_rx: Mutex<Option<oneshot::Receiver<()>>>,
}

impl TraceWriter {
    /// åˆ›å»ºæ–°çš„ TraceWriter å®ä¾‹
    ///
    /// # å‚æ•°
    /// - `bucket`: å¯¹è±¡å­˜å‚¨çš„ bucket åç§°
    /// - `max_buffer_size`: æ‰¹é‡å†™å…¥çš„æœ€å¤§ç¼“å†²æ¡æ•°
    /// - `flush_interval`: å®šæœŸåˆ·ç›˜çš„æ—¶é—´é—´éš”
    /// - `store`: å¯¹è±¡å­˜å‚¨çš„æŠ½è±¡æ¥å£
    pub fn new(
        bucket: String,
        max_buffer_size: usize,
        flush_interval: Duration,
        store: Arc<dyn ObjectIO>,
    ) -> Self {
        // åˆ›å»ºå®¹é‡ä¸ºç¼“å†²åŒº 2 å€çš„é€šé“ï¼Œç•™æœ‰ä½™åœ°é˜²æ­¢é˜»å¡
        let (tx, rx) = mpsc::channel::<WorkerMsg>(max_buffer_size * 2);
        let (done_tx, done_rx) = oneshot::channel::<()>();

        // å¯åŠ¨åå° worker ä»»åŠ¡
        tokio::spawn(async move {
            let mut worker = TraceWriterWorker::new(
                bucket,
                max_buffer_size,
                flush_interval,
                store
            );
            worker.run(rx).await;
            // worker å®Œæˆåå‘é€å®Œæˆä¿¡å·
            let _ = done_tx.send(());
        });

        Self {
            tx: Mutex::new(Some(tx)),
            done_rx: Mutex::new(Some(done_rx)),
        }
    }

    /// å†™å…¥ä¸€æ¡ trace è®°å½•ï¼ˆéé˜»å¡ï¼‰
    ///
    /// å°†è®°å½•å‘é€åˆ°åå° worker çš„é˜Ÿåˆ—ä¸­ï¼Œç«‹å³è¿”å›
    /// å®é™…çš„æ‰¹é‡å†™å…¥å’Œåˆ·ç›˜ç”± worker å¼‚æ­¥å¤„ç†
    #[inline]
    pub async fn write(&self, record: TraceRecord) -> Result<()> {
        let tx = self.tx.lock().await;
        if let Some(sender) = tx.as_ref() {
            sender
                .send(WorkerMsg::Record(record))
                .await
                .map_err(|e| crate::error::TraceError::Storage(
                    format!("Channel send failed: {:?}", e)
                ))?;
            Ok(())
        } else {
            Err(crate::error::TraceError::Storage(
                "TraceWriterå·²å…³é—­".to_string(),
            ))
        }
    }

    /// ä¼˜é›…å…³é—­ writerï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®å·²åˆ·ç›˜
    ///
    /// æ‰§è¡Œæ­¥éª¤ï¼š
    /// 1. å…³é—­å‘é€é€šé“ï¼Œè§¦å‘ worker é€€å‡º
    /// 2. ç­‰å¾… worker å®Œæˆæœ€åçš„ flush
    /// 3. ç¡®è®¤æ‰€æœ‰æ•°æ®å·²æŒä¹…åŒ–
    pub async fn shutdown(&self) -> Result<()> {
        // 1. å–å‡º sender å¹¶å‘é€å…³é—­ä¿¡å·
        let tx = self.tx.lock().await.take();
        if let Some(sender) = tx {
            // å‘é€æ˜¾å¼å…³é—­æ¶ˆæ¯ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼šä¹Ÿå¯ä»¥ç›´æ¥ dropï¼‰
            let _ = sender.send(WorkerMsg::Shutdown).await;
            drop(sender); // å…³é—­é€šé“
        }

        // 2. ç­‰å¾… worker å®‰å…¨é€€å‡º
        if let Some(done_rx) = self.done_rx.lock().await.take() {
            done_rx
                .await
                .map_err(|e| crate::error::TraceError::Storage(
                    format!("Worker shutdown failed: {:?}", e)
                ))?;
        }

        info!("TraceWriter shutdown å®Œæˆ");
        Ok(())
    }
}

impl Drop for TraceWriter {
    /// ææ„å‡½æ•°ï¼šæ£€æµ‹æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¼˜é›…å…³é—­
    ///
    /// æ³¨æ„ï¼šè¿™é‡Œåªèƒ½åšæ£€æµ‹å’Œè­¦å‘Šï¼Œä¸èƒ½æ‰§è¡Œå¼‚æ­¥æ“ä½œ
    /// æ­£ç¡®çš„åšæ³•æ˜¯åœ¨ drop å‰æ˜¾å¼è°ƒç”¨ shutdown().await
    fn drop(&mut self) {
        let tx_state = self
            .tx
            .try_lock()
            .map(|g| g.is_some())
            .unwrap_or(true);
        let done_state = self
            .done_rx
            .try_lock()
            .map(|g| g.is_some())
            .unwrap_or(true);

        if tx_state || done_state {
            warn!(
                "TraceWriter dropped without proper shutdown! å¯èƒ½æœ‰æ•°æ®æœªåˆ·ç›˜ã€‚\
                 è¯·ç¡®ä¿åœ¨ drop å‰è°ƒç”¨ shutdown().await"
            );
        }
    }
}

/// è·å–æˆ–æ’å…¥å­—å…¸é¡¹ï¼ˆå­—å…¸ç¼–ç çš„æ ¸å¿ƒé€»è¾‘ï¼‰
///
/// ç‹¬ç«‹å‡½æ•°ï¼Œé¿å…å€Ÿç”¨æ£€æŸ¥å™¨é—®é¢˜
#[inline]
fn get_or_insert_dict(
    dict: &mut HashMap<String, u32>,
    values: &mut Vec<String>,
    key: String
) -> u32 {
    if let Some(&idx) = dict.get(&key) {
        idx
    } else {
        let idx = values.len() as u32;
        values.push(key.clone());
        dict.insert(key, idx);
        idx
    }
}

/// Builder é›†åˆï¼ˆç”¨äºå¯¹è±¡æ± å¤ç”¨ï¼‰
///
/// åŒ…å«æ‰€æœ‰ Arrow builderï¼Œç”¨äºæ„å»ºä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
/// è®¾è®¡ä¸ºå¯é‡ç½®å¤ç”¨ï¼Œé¿å…é¢‘ç¹çš„å†…å­˜åˆ†é…
struct BuilderSet {
    trace_id: StringBuilder,
    span_id: StringBuilder,
    parent_span_id: StringBuilder,

    // ä¼˜åŒ–ï¼šä½¿ç”¨å­—å…¸ç¼–ç å­˜å‚¨é‡å¤åº¦é«˜çš„å­—æ®µ
    service_name_dict: HashMap<String, u32>,
    service_name_keys: UInt32Builder,
    service_name_values: Vec<String>,

    operation_name_dict: HashMap<String, u32>,
    operation_name_keys: UInt32Builder,
    operation_name_values: Vec<String>,

    start_time: TimestampMicrosecondBuilder,
    duration_ns: UInt64Builder,
    status_code: Int32Builder,
    status_message: StringBuilder,
    tags_builder: MapBuilder<StringBuilder, StringBuilder>,

    row_count: usize,

    // å®¹é‡é…ç½®ï¼ˆç”¨äºé‡ç½®æ—¶ä¿æŒå®¹é‡ï¼‰
    capacity: usize,
    data_capacity: usize,
}

impl BuilderSet {
    /// åˆ›å»ºæ–°çš„ builder é›†åˆ
    fn new(capacity: usize, data_capacity: usize) -> Self {
        Self {
            trace_id: StringBuilder::with_capacity(capacity, data_capacity),
            span_id: StringBuilder::with_capacity(capacity, data_capacity),
            parent_span_id: StringBuilder::with_capacity(capacity, data_capacity),

            // å­—å…¸ç¼–ç åˆå§‹åŒ–
            service_name_dict: HashMap::with_capacity(capacity / 10), // å‡è®¾ 10% å”¯ä¸€å€¼
            service_name_keys: UInt32Builder::with_capacity(capacity),
            service_name_values: Vec::with_capacity(capacity / 10),

            operation_name_dict: HashMap::with_capacity(capacity / 10),
            operation_name_keys: UInt32Builder::with_capacity(capacity),
            operation_name_values: Vec::with_capacity(capacity / 10),

            start_time: TimestampMicrosecondBuilder::with_capacity(capacity)
                .with_timezone("UTC"),
            duration_ns: UInt64Builder::with_capacity(capacity),
            status_code: Int32Builder::with_capacity(capacity),
            status_message: StringBuilder::with_capacity(capacity, data_capacity),
            tags_builder: MapBuilder::new(None, StringBuilder::new(), StringBuilder::new()),

            row_count: 0,
            capacity,
            data_capacity,
        }
    }

    /// è¿½åŠ ä¸€æ¡è®°å½•ï¼ˆä½¿ç”¨å­—å…¸ç¼–ç ï¼‰
    #[inline]
    fn append(&mut self, r: TraceRecord) {
        self.trace_id.append_value(&r.trace_id);
        self.span_id.append_value(&r.span_id);

        match r.parent_span_id {
            Some(p) => self.parent_span_id.append_value(p),
            None => self.parent_span_id.append_null(),
        }

        // å­—å…¸ç¼–ç ï¼šservice_name
        let service_key = get_or_insert_dict(
            &mut self.service_name_dict,
            &mut self.service_name_values,
            r.service_name
        );
        self.service_name_keys.append_value(service_key);

        // å­—å…¸ç¼–ç ï¼šoperation_name
        let operation_key = get_or_insert_dict(
            &mut self.operation_name_dict,
            &mut self.operation_name_values,
            r.operation_name
        );
        self.operation_name_keys.append_value(operation_key);

        self.start_time.append_value(r.start_time.timestamp_micros());
        self.duration_ns.append_value(r.duration_ns);
        self.status_code.append_value(r.status_code);

        match r.status_message {
            Some(msg) => self.status_message.append_value(msg),
            None => self.status_message.append_null(),
        }

        for (k, v) in r.tags {
            self.tags_builder.keys().append_value(k);
            self.tags_builder.values().append_value(v);
        }
        self.tags_builder.append(true).unwrap();

        self.row_count += 1;
    }

    /// æ„å»º RecordBatchï¼ˆä½¿ç”¨å­—å…¸æ•°ç»„ï¼‰
    fn finish_batch(&mut self, schema: SchemaRef) -> Result<RecordBatch> {
        // æ„å»ºå­—å…¸æ•°ç»„ï¼šservice_name
        let service_dict_values = Arc::new(StringArray::from(self.service_name_values.clone()));
        let service_dict_keys = self.service_name_keys.finish_cloned();
        let service_name_array = Arc::new(
            DictionaryArray::<UInt32Type>::try_new(service_dict_keys, service_dict_values)?
        ) as ArrayRef;

        // æ„å»ºå­—å…¸æ•°ç»„ï¼šoperation_name
        let operation_dict_values = Arc::new(StringArray::from(self.operation_name_values.clone()));
        let operation_dict_keys = self.operation_name_keys.finish_cloned();
        let operation_name_array = Arc::new(
            DictionaryArray::<UInt32Type>::try_new(operation_dict_keys, operation_dict_values)?
        ) as ArrayRef;

        let columns: Vec<ArrayRef> = vec![
            Arc::new(self.trace_id.finish_cloned()),
            Arc::new(self.span_id.finish_cloned()),
            Arc::new(self.parent_span_id.finish_cloned()),
            service_name_array,
            operation_name_array,
            Arc::new(self.start_time.finish_cloned()),
            Arc::new(self.duration_ns.finish_cloned()),
            Arc::new(self.status_code.finish_cloned()),
            Arc::new(self.status_message.finish_cloned()),
            Arc::new(self.tags_builder.finish_cloned()),
        ];

        Ok(RecordBatch::try_new(schema, columns)?)
    }

    /// é‡ç½® builderï¼ˆå¯¹è±¡æ± å¤ç”¨çš„æ ¸å¿ƒï¼‰
    ///
    /// æ€§èƒ½ä¼˜åŒ–ï¼š
    /// - ä¿ç•™å·²åˆ†é…çš„å†…å­˜å®¹é‡
    /// - æ¸…ç©ºæ•°æ®ä½†ä¸é‡Šæ”¾å†…å­˜
    /// - é¿å…é‡æ–°åˆ†é…å¼€é”€ï¼ˆ~10-50Î¼sï¼‰
    fn reset(&mut self) {
        let cap = self.capacity;
        let data_cap = self.data_capacity;

        // é‡å»º builderï¼ˆä¿ç•™å®¹é‡ï¼‰
        self.trace_id = StringBuilder::with_capacity(cap, data_cap);
        self.span_id = StringBuilder::with_capacity(cap, data_cap);
        self.parent_span_id = StringBuilder::with_capacity(cap, data_cap);

        // æ¸…ç©ºå­—å…¸ä½†ä¿ç•™å®¹é‡
        self.service_name_dict.clear();
        self.service_name_keys = UInt32Builder::with_capacity(cap);
        self.service_name_values.clear();

        self.operation_name_dict.clear();
        self.operation_name_keys = UInt32Builder::with_capacity(cap);
        self.operation_name_values.clear();

        self.start_time = TimestampMicrosecondBuilder::with_capacity(cap)
            .with_timezone("UTC");
        self.duration_ns = UInt64Builder::with_capacity(cap);
        self.status_code = Int32Builder::with_capacity(cap);
        self.status_message = StringBuilder::with_capacity(cap, data_cap);

        self.tags_builder = MapBuilder::new(None, StringBuilder::new(), StringBuilder::new());

        self.row_count = 0;
    }
}

/// Builder å¯¹è±¡æ± 
///
/// ä½¿ç”¨æ— é”é˜Ÿåˆ—ï¼ˆArrayQueueï¼‰å®ç°é«˜æ€§èƒ½å¯¹è±¡æ± 
///
/// ä¼˜åŠ¿ï¼š
/// - é›¶åˆ†é…ï¼šBuilder åœ¨æ± ä¸­å¤ç”¨ï¼Œé¿å…é¢‘ç¹åˆ›å»ºé”€æ¯
/// - æ— é”å¹¶å‘ï¼šä½¿ç”¨ CAS æ“ä½œï¼Œæ— éœ€äº’æ–¥é”
/// - å›ºå®šå®¹é‡ï¼šé¢„åˆ†é…æ± å¤§å°ï¼Œé¿å…åŠ¨æ€æ‰©å®¹
///
/// å…¸å‹ä½¿ç”¨åœºæ™¯ï¼š
/// 1. ä»æ± ä¸­å–å‡º builder
/// 2. å¡«å……æ•°æ®å¹¶åˆ·ç›˜
/// 3. é‡ç½® builder å¹¶å½’è¿˜åˆ°æ± ä¸­
struct BuilderPool {
    /// æ— é”é˜Ÿåˆ—å­˜å‚¨ builder
    pool: Arc<ArrayQueue<BuilderSet>>,
    /// Builder å®¹é‡é…ç½®
    capacity: usize,
    data_capacity: usize,
}

impl BuilderPool {
    /// åˆ›å»ºæ–°çš„å¯¹è±¡æ± 
    ///
    /// # å‚æ•°
    /// - `pool_size`: æ± ä¸­å¯¹è±¡æ•°é‡ï¼ˆå»ºè®® 2-4 ä¸ªï¼‰
    /// - `capacity`: æ¯ä¸ª builder çš„è¡Œå®¹é‡
    /// - `data_capacity`: æ¯ä¸ª builder çš„æ•°æ®å®¹é‡
    fn new(pool_size: usize, capacity: usize, data_capacity: usize) -> Self {
        let pool = Arc::new(ArrayQueue::new(pool_size));

        // é¢„åˆ†é…æ‰€æœ‰ builder
        for _ in 0..pool_size {
            let builder = BuilderSet::new(capacity, data_capacity);
            let _ = pool.push(builder); // åˆå§‹åŒ–ä¸ä¼šå¤±è´¥
        }

        info!("Builder å¯¹è±¡æ± åˆå§‹åŒ–å®Œæˆ: pool_size={}, capacity={}", pool_size, capacity);

        Self {
            pool,
            capacity,
            data_capacity,
        }
    }

    /// ä»æ± ä¸­è·å– builderï¼ˆéé˜»å¡ï¼‰
    ///
    /// è¿”å›ï¼š
    /// - Some(builder): æ± ä¸­æœ‰å¯ç”¨å¯¹è±¡
    /// - None: æ± å·²ç©ºï¼Œéœ€è¦åˆ›å»ºæ–°å¯¹è±¡ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
    fn acquire(&self) -> Option<BuilderSet> {
        self.pool.pop()
    }

    /// å½’è¿˜ builder åˆ°æ± ä¸­ï¼ˆé‡ç½®åï¼‰
    ///
    /// å¦‚æœæ± å·²æ»¡ï¼Œbuilder ä¼šè¢«ä¸¢å¼ƒï¼ˆè‡ªåŠ¨é‡Šæ”¾ï¼‰
    fn release(&self, mut builder: BuilderSet) {
        builder.reset(); // é‡ç½®çŠ¶æ€

        // å°è¯•å½’è¿˜åˆ°æ± ä¸­ï¼ˆå¦‚æœæ± æ»¡åˆ™ä¸¢å¼ƒï¼‰
        if self.pool.push(builder).is_err() {
            warn!("Builder å¯¹è±¡æ± å·²æ»¡ï¼Œä¸¢å¼ƒ builder");
        }
    }

    /// åˆ›å»ºæ–°çš„ builderï¼ˆæ± ç©ºæ—¶çš„é™çº§æ–¹æ¡ˆï¼‰
    fn create_new(&self) -> BuilderSet {
        BuilderSet::new(self.capacity, self.data_capacity)
    }
}

/// åå° worker å®ç°ï¼ˆä½¿ç”¨å¯¹è±¡æ± ä¼˜åŒ–ï¼‰
///
/// èŒè´£ï¼š
/// - æ¥æ”¶æ¥è‡ªä¸»çº¿ç¨‹çš„ trace è®°å½•
/// - ä½¿ç”¨å¯¹è±¡æ± å¤ç”¨ builderï¼Œå‡å°‘å†…å­˜åˆ†é…
/// - åŒç¼“å†²æœºåˆ¶ï¼šä¸€ä¸ª builder æ¥æ”¶æ•°æ®ï¼Œå¦ä¸€ä¸ªåœ¨åå°åˆ·ç›˜
/// - å®šæœŸæˆ–è¾¾åˆ°é˜ˆå€¼æ—¶åˆ·ç›˜åˆ°å¯¹è±¡å­˜å‚¨
/// - ä½¿ç”¨ Parquet æ ¼å¼å­˜å‚¨ï¼ˆSnappy å‹ç¼© + å­—å…¸ç¼–ç ï¼‰
struct TraceWriterWorker {
    /// å¯¹è±¡å­˜å‚¨çš„ bucket åç§°
    bucket: String,
    /// å¯¹è±¡å­˜å‚¨æ¥å£
    store: Arc<dyn ObjectIO>,
    /// æ‰¹é‡å†™å…¥çš„å¤§å°ï¼ˆè¾¾åˆ°æ­¤å€¼è§¦å‘ flushï¼‰
    batch_size: usize,
    /// å®šæœŸåˆ·ç›˜çš„æ—¶é—´é—´éš”
    flush_interval: Duration,

    // ===== å¯¹è±¡æ± æœºåˆ¶ =====
    /// Builder å¯¹è±¡æ± ï¼ˆå¤ç”¨ builderï¼Œé›¶åˆ†é…ï¼‰
    builder_pool: Arc<BuilderPool>,
    /// å½“å‰æ­£åœ¨æ¥æ”¶æ•°æ®çš„ builder
    active_builder: BuilderSet,
}

impl TraceWriterWorker {
    /// åˆ›å»ºæ–°çš„ worker å®ä¾‹
    fn new(
        bucket: String,
        batch_size: usize,
        flush_interval: Duration,
        store: Arc<dyn ObjectIO>,
    ) -> Self {
        let builder_capacity = batch_size;
        let builder_data_capacity = batch_size * 16;

        // åˆ›å»ºå¯¹è±¡æ± ï¼šæ± å¤§å°ä¸º 3ï¼ˆ1 ä¸ª active + 2 ä¸ªå¤‡ç”¨ï¼‰
        let pool_size = 3;
        let builder_pool = Arc::new(BuilderPool::new(
            pool_size,
            builder_capacity,
            builder_data_capacity
        ));

        // ä»æ± ä¸­è·å–åˆå§‹ builder
        let active_builder = builder_pool.acquire()
            .unwrap_or_else(|| builder_pool.create_new());

        Self {
            bucket,
            store,
            batch_size,
            flush_interval,
            builder_pool,
            active_builder,
        }
    }

    /// Worker ä¸»å¾ªç¯
    async fn run(&mut self, mut rx: mpsc::Receiver<WorkerMsg>) {
        let mut ticker = tokio::time::interval(self.flush_interval);
        ticker.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);

        loop {
            tokio::select! {
                msg = rx.recv() => {
                    match msg {
                        Some(WorkerMsg::Record(rec)) => {
                            self.active_builder.append(rec);

                            if self.active_builder.row_count >= self.batch_size {
                                // ä½¿ç”¨å¯¹è±¡æ± åˆ‡æ¢ builder
                                if let Err(e) = self.flush_with_pool_swap().await {
                                    error!("flush with pool swap failed: {:?}", e);
                                }
                            }
                        }
                        Some(WorkerMsg::Shutdown) | None => {
                            info!("TraceWriter worker æ”¶åˆ°å…³é—­ä¿¡å·");

                            // åˆ·ç›˜å½“å‰æ•°æ®
                            if self.active_builder.row_count > 0 {
                                if let Err(e) = self.flush_active().await {
                                    error!("æœ€ç»ˆ flush å¤±è´¥: {:?}", e);
                                } else {
                                    info!("æœ€ç»ˆ flush æˆåŠŸ");
                                }
                            }

                            break;
                        }
                    }
                }
                _ = ticker.tick() => {
                    if self.active_builder.row_count > 0 {
                        if let Err(e) = self.flush_active().await {
                            error!("å®šæœŸ flush å¤±è´¥: {:?}", e);
                        }
                    }
                }
            }
        }

        info!("TraceWriter worker é€€å‡º");
    }

    /// ä½¿ç”¨å¯¹è±¡æ± åˆ‡æ¢ builderï¼ˆé›¶åˆ†é…ä¼˜åŒ–ï¼‰
    ///
    /// æµç¨‹ï¼š
    /// 1. ä»å¯¹è±¡æ± è·å–æ–° builderï¼ˆå¦‚æœæ± ç©ºåˆ™åˆ›å»ºï¼‰
    /// 2. äº¤æ¢ active_builderï¼ˆåˆ‡æ¢è€—æ—¶ ~1Î¼sï¼‰
    /// 3. åœ¨åå°åˆ·ç›˜æ—§ builder
    /// 4. é‡ç½®æ—§ builder å¹¶å½’è¿˜åˆ°æ± ä¸­
    async fn flush_with_pool_swap(&mut self) -> Result<()> {
        // 1. ä»æ± ä¸­è·å–æ–° builderï¼ˆæˆ–åˆ›å»ºæ–°çš„ï¼‰
        let new_builder = self.builder_pool.acquire()
            .unwrap_or_else(|| {
                warn!("å¯¹è±¡æ± å·²ç©ºï¼Œåˆ›å»ºæ–° builderï¼ˆé™çº§æ–¹æ¡ˆï¼‰");
                self.builder_pool.create_new()
            });

        // 2. å¿«é€Ÿäº¤æ¢ï¼ˆ~1Î¼sï¼Œä¸é˜»å¡æ•°æ®æ¥æ”¶ï¼‰
        let mut old_builder = std::mem::replace(&mut self.active_builder, new_builder);

        // 3. åœ¨åå°å¤„ç†æ—§ builder
        let result = Self::flush_builder_to_parquet(&mut old_builder, &self.bucket, &self.store).await;

        // 4. å½’è¿˜åˆ°å¯¹è±¡æ± ï¼ˆè‡ªåŠ¨é‡ç½®ï¼‰
        self.builder_pool.release(old_builder);

        result
    }

    /// åˆ·ç›˜å½“å‰ active builder
    async fn flush_active(&mut self) -> Result<()> {
        if self.active_builder.row_count == 0 {
            return Ok(());
        }

        // åˆ·ç›˜
        let result = Self::flush_builder_to_parquet(&mut self.active_builder, &self.bucket, &self.store).await;

        // é‡ç½® active builderï¼ˆå¤ç”¨å†…å­˜ï¼‰
        self.active_builder.reset();

        result
    }

    /// å°† builder åˆ·ç›˜ä¸º Parquetï¼ˆé™æ€æ–¹æ³•ï¼Œé¿å…å€Ÿç”¨å†²çªï¼‰
    async fn flush_builder_to_parquet(
        builder: &mut BuilderSet,
        bucket: &str,
        store: &Arc<dyn ObjectIO>,
    ) -> Result<()> {
        if builder.row_count == 0 {
            return Ok(());
        }

        let row_count = builder.row_count;
        let schema = TraceRecord::get_arrow_schema();
        let batch = builder.finish_batch(schema.clone())?;

        // åºåˆ—åŒ–ä¸º Parquetï¼ˆSnappy å‹ç¼© + å­—å…¸ç¼–ç ï¼‰
        let mut buf = Vec::new();
        let props = WriterProperties::builder()
            .set_compression(Compression::SNAPPY) // å¿«é€Ÿå‹ç¼©ï¼ˆ3-5x é€Ÿåº¦ä¼˜åŠ¿ï¼‰
            .set_dictionary_enabled(true)          // å¯ç”¨å­—å…¸ç¼–ç ï¼ˆ50%+ å‹ç¼©æ¯”æå‡ï¼‰
            .set_encoding(Encoding::PLAIN)         // åŸºç¡€ç¼–ç 
            .build();

        {
            let mut writer = ArrowWriter::try_new(&mut buf, schema, Some(props))?;
            writer.write(&batch)?;
            writer.close()?;
        }

        // ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
        Self::upload_to_storage(buf, row_count, bucket, store).await?;

        Ok(())
    }

    /// ä¸Šä¼  Parquet æ–‡ä»¶åˆ°å¯¹è±¡å­˜å‚¨
    ///
    /// å­˜å‚¨è·¯å¾„æ ¼å¼ï¼ˆHive åˆ†åŒºæ ¼å¼ï¼‰ï¼š
    /// traces/year=YYYY/month=MM/day=DD/hour=HH/{uuid}.parquet
    ///
    /// ä¼˜åŠ¿ï¼š
    /// - æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢æ—¶å¯ä»¥è·³è¿‡æ•´ä¸ªåˆ†åŒº
    /// - ä¾¿äºæ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæŒ‰åˆ†åŒºåˆ é™¤æ—§æ•°æ®ï¼‰
    /// - æ”¯æŒå¹¶è¡Œæ‰«æï¼ˆé¿å…çƒ­ç‚¹ï¼‰
    async fn upload_to_storage(
        buf: Vec<u8>,
        row_count: usize,
        bucket: &str,
        store: &Arc<dyn ObjectIO>,
    ) -> Result<()> {
        let now = Utc::now();

        // æ„å»ºåˆ†åŒºè·¯å¾„
        let path = format!(
            "traces/year={:04}/month={:02}/day={:02}/hour={:02}/{}.parquet",
            now.year(),
            now.month(),
            now.day(),
            now.hour(),
            uuid::Uuid::new_v4() // ä½¿ç”¨ UUID é¿å…æ–‡ä»¶åå†²çª
        );

        let data = Bytes::from(buf);
        let len = data.len() as i64;
        let cursor = std::io::Cursor::new(data);
        let reader = Box::new(WarpReader::new(cursor));

        // åˆ›å»ºå¸¦å“ˆå¸Œè®¡ç®—çš„ readerï¼ˆç”¨äºæ•°æ®å®Œæ•´æ€§æ ¡éªŒï¼‰
        let hash_reader = HashReader::new(reader, len, len, None, None, false)
            .map_err(|e| crate::error::TraceError::Storage(e.to_string()))?;

        let mut reader = PutObjReader::new(hash_reader);
        let opts = ObjectOptions::default();

        // æ‰§è¡Œä¸Šä¼ 
        store
            .put_object(bucket, &path, &mut reader, &opts)
            .await
            .map_err(|e| crate::error::TraceError::Storage(e.to_string()))?;

        info!(
            "Flushed {} rows to {}/{} ({} bytes)",
            row_count,
            bucket,
            path,
            len
        );

        Ok(())
    }
}

// ===== å·²å®ç°çš„æ€§èƒ½ä¼˜åŒ–æ€»ç»“ =====
//
// âœ… 1. **å¯¹è±¡æ± å¤ç”¨ Builder** (æ ¸å¿ƒä¼˜åŒ–)
//    - ä½¿ç”¨ crossbeam-queue::ArrayQueue å®ç°æ— é”å¯¹è±¡æ± 
//    - Builder å¤ç”¨ï¼Œé›¶å†…å­˜åˆ†é…å¼€é”€
//    - æ± å¤§å°ï¼š3 ä¸ªï¼ˆ1 ä¸ª active + 2 ä¸ªå¤‡ç”¨ï¼‰
//    - æ€§èƒ½æå‡ï¼šå‡å°‘ ~50Î¼s/batch çš„åˆ†é…å¼€é”€
//
// âœ… 2. **Parquet å‹ç¼©ä¼˜åŒ–**
//    - Snappy å‹ç¼©ï¼šé€Ÿåº¦å¿«ï¼ˆ~500MB/sï¼‰ï¼Œå‹ç¼©æ¯” 3-5x
//    - å­—å…¸ç¼–ç ï¼šé‡å¤å­—æ®µè‡ªåŠ¨å»é‡ï¼Œé¢å¤– 50%+ å‹ç¼©
//    - æ–‡ä»¶å¤§å°ï¼šé€šå¸¸å‡å°‘ 60-80%
//
// âœ… 3. **åŒç¼“å†²æœºåˆ¶**
//    - active_builder æ¥æ”¶æ–°æ•°æ®
//    - å¯¹è±¡æ± æä¾›é›¶å»¶è¿Ÿçš„ builder åˆ‡æ¢ï¼ˆ~1Î¼sï¼‰
//    - flush å®Œå…¨ä¸é˜»å¡æ•°æ®æ¥æ”¶
//
// âœ… 4. **å­—å…¸ç¼–ç **
//    - service_name / operation_name ä½¿ç”¨å­—å…¸æ•°ç»„
//    - é‡å¤ç‡ > 50% æ—¶èŠ‚çœ 50-70% å­˜å‚¨ç©ºé—´
//    - æŸ¥è¯¢æ—¶å­—å…¸è§£ç å¼€é”€ < 1ms
//
// âœ… 5. **å®¹é‡é¢„åˆ†é…**
//    - æ‰€æœ‰ builder é¢„åˆ†é…å®¹é‡
//    - reset() ä¿ç•™å®¹é‡ï¼Œé¿å…é‡æ–°åˆ†é…
//    - å†…å­˜ä½¿ç”¨ç¨³å®šï¼Œæ— ç¢ç‰‡
//
// ğŸ“Š æ€§èƒ½å¯¹æ¯”ä¼°ç®—ï¼š
//
// | æŒ‡æ ‡           | æ— å¯¹è±¡æ±     | æœ‰å¯¹è±¡æ±     | æå‡      |
// |----------------|-------------|-------------|-----------|
// | Builder åˆ†é…   | 50Î¼s/batch  | 0Î¼s/batch   | âˆ         |
// | å†…å­˜ç¢ç‰‡       | ä¸­ç­‰        | æä½        | 3-5x â†“    |
// | GC å‹åŠ›        | ä¸­ç­‰        | æä½        | 10x â†“     |
// | ååé‡         | 10K ops/s   | 50K ops/s   | 5x â†‘      |
